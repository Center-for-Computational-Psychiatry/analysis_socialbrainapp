{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, json, pickle, os, warnings\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "\n",
    "from socialbrainapp import get_demographics, get_oci, get_sds, get_lsas, get_hardball, get_hardball_ratings, get_journey\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 jsons to parse\n"
     ]
    }
   ],
   "source": [
    "# Get today's date\n",
    "todays_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "# Load json data\n",
    "json_dir = '../json'\n",
    "out_dir = '../data/runs'\n",
    "json_files = glob(f'{json_dir}/*json')\n",
    "print(f'{len(json_files)} jsons to parse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse json data\n",
    "id_lists = {'Demographics':[], 'Hardball':[], 'HardballSubjectiveRatings':[], \n",
    "            'OCI':[], 'SDS':[], 'LSAS':[], \n",
    "            'Journey_decisions':[], 'Journey_memory':[], 'Journey_dots':[], 'Journey_characters':[]}\n",
    "\n",
    "df_lists = {'Demographics':[], 'Hardball':[], 'HardballSubjectiveRatings':[], \n",
    "            'OCI':[], 'SDS':[], 'LSAS':[],\n",
    "            'Journey_decisions':[], 'Journey_memory':[], 'Journey_dots':[], 'Journey_characters':[]}\n",
    "\n",
    "for json_file in json_files:\n",
    "    \n",
    "    try:\n",
    "        with open(json_file, 'rb') as handle:\n",
    "            tmp_data = json.load(handle)\n",
    "\n",
    "        for element in tmp_data:\n",
    "\n",
    "            # Get Demographics\n",
    "            if 'Age' in element.keys():\n",
    "                id_lists['Demographics'] += [element['UserId']]\n",
    "                df_lists['Demographics'] += [get_demographics(element)]\n",
    "\n",
    "            # Get Survey Data\n",
    "            if 'SurveyName' in element.keys():\n",
    "\n",
    "                # Get OCI\n",
    "                if element['SurveyName'] == 'OCI':\n",
    "                    id_lists['OCI'] += [element['UserId']]\n",
    "                    df_lists['OCI'] += [get_oci(element)]\n",
    "\n",
    "                # Get SDS\n",
    "                elif element['SurveyName'] == 'SDS':\n",
    "                    id_lists['SDS'] += [element['UserId']]\n",
    "                    df_lists['SDS'] += [get_sds(element)]\n",
    "\n",
    "                # Get LSAS\n",
    "                elif element['SurveyName'] == 'LSAS':\n",
    "                    id_lists['LSAS'] += [element['UserId']]\n",
    "                    df_lists['LSAS'] += [get_lsas(element)]\n",
    "\n",
    "            # Get Game Data\n",
    "            if 'Game' in element.keys():\n",
    "                \n",
    "                # Get Hardball\n",
    "                if element['Game'] == 'Hardball':\n",
    "                    if 'Screen' not in element.keys():\n",
    "                        id_lists['Hardball'] += [element['UserId']]\n",
    "                        df_lists['Hardball'] += [get_hardball(element)]\n",
    "\n",
    "                    if 'Screen' in element.keys():\n",
    "                        id_lists['HardballSubjectiveRatings'] += [element['UserId']]\n",
    "                        df_lists['HardballSubjectiveRatings'] += [get_hardball_ratings(element)]\n",
    "\n",
    "                # Get Journey\n",
    "                if element['Game'] == 'Journey':\n",
    "                    \n",
    "                    try: \n",
    "                        task_name, snt_df = get_journey(element)\n",
    "                        id_lists['Journey_'+task_name] += [element['UserId']]\n",
    "                        df_lists['Journey_'+task_name] += [snt_df]\n",
    "                        \n",
    "                    except: # not interested in some trials but want a better solution here\n",
    "                        continue\n",
    "\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# concatenate all df lists\n",
    "full_dfs = {}\n",
    "for (key, value) in df_lists.items():\n",
    "    if len(value) > 0: \n",
    "        full_dfs[key] = []\n",
    "        full_dfs[key] = pd.concat(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Data\n",
    "preproc_dfs = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309399CE-63A7-472E-8619-65576780E6A2 180 trials\n",
      "405F5765-F99C-4A61-9D21-55C4BCD818B0 94 trials\n",
      "6CC2F911-589F-40F5-B27E-460291076521 120 trials\n",
      "8312DACF-5F5F-4BEC-9AC8-2F3CE0D0C47E 82 trials\n",
      "969B718B-CD49-47E2-BA31-56643220BC84 120 trials\n",
      "D832F3A0-C39F-4BA5-9F13-4C1FD8966159 120 trials\n",
      "No influence rating for 969B718B-CD49-47E2-BA31-56643220BC84\n",
      "No influence rating for 309399CE-63A7-472E-8619-65576780E6A2\n",
      "No influence rating for 6CC2F911-589F-40F5-B27E-460291076521\n",
      "No influence rating for 405F5765-F99C-4A61-9D21-55C4BCD818B0\n",
      "No influence rating for 8312DACF-5F5F-4BEC-9AC8-2F3CE0D0C47E\n"
     ]
    }
   ],
   "source": [
    "# remove Hardball subjects with less than 60 completed trials\n",
    "preproc_dfs['Hardball'] = full_dfs['Hardball'].copy()\n",
    "preproc_dfs['HardballSubjectiveRatings'] = full_dfs['HardballSubjectiveRatings'].copy()\n",
    "\n",
    "for subj_id in np.unique(full_dfs['Hardball'].index):\n",
    "    ntrials = len(full_dfs['Hardball'][full_dfs['Hardball'].index==subj_id])\n",
    "    if ntrials < 60:\n",
    "        preproc_dfs['Hardball'] = preproc_dfs['Hardball'].drop(subj_id)\n",
    "        try:\n",
    "            preproc_dfs['HardballSubjectiveRatings'] = preproc_dfs['HardballSubjectiveRatings'].drop(subj_id)\n",
    "        except:\n",
    "            continue\n",
    "    elif ntrials > 60:\n",
    "        print(subj_id, f'{ntrials} trials')\n",
    "        preproc_dfs['Hardball'].at[subj_id, 'NTrials'] = ntrials\n",
    "        preproc_dfs['Hardball'].at[subj_id, 'Include'] = np.nan\n",
    "    else:\n",
    "        preproc_dfs['Hardball'].at[subj_id, 'NTrials'] = ntrials\n",
    "        preproc_dfs['Hardball'].at[subj_id, 'Include'] = 1\n",
    "\n",
    "preproc_dfs['HardballSubjectiveRatings'].to_csv(f'{out_dir}/HardballSubjectiveRatings-data-{todays_date}.csv', index_label='SubjectID')\n",
    "\n",
    "# combine dfs\n",
    "text_to_display = []\n",
    "preproc_dfs['Hardball'] = preproc_dfs['Hardball'].reset_index()\n",
    "\n",
    "for idx in preproc_dfs['Hardball'].index:\n",
    "    subj_id = preproc_dfs['Hardball']['index'][idx]\n",
    "    team_id = preproc_dfs['Hardball']['TeamName'][idx]\n",
    "    year_id = preproc_dfs['Hardball']['Year'][idx]\n",
    "    month_id = preproc_dfs['Hardball']['Month'][idx]\n",
    "    day_id = preproc_dfs['Hardball']['Day'][idx]\n",
    "\n",
    "    try:\n",
    "        influence_rating = float(preproc_dfs['HardballSubjectiveRatings'][(preproc_dfs['HardballSubjectiveRatings']['TeamName']==team_id) & (preproc_dfs['HardballSubjectiveRatings']['Year']==year_id) & (preproc_dfs['HardballSubjectiveRatings']['Month']==month_id) & (preproc_dfs['HardballSubjectiveRatings']['Day']==day_id)].at[subj_id, 'Rate'])\n",
    "        preproc_dfs['Hardball'].at[idx, 'InfluenceRating'] = influence_rating\n",
    "    except:\n",
    "        if f'No influence rating for {subj_id}' not in text_to_display:\n",
    "            text_to_display += [f'No influence rating for {subj_id}']\n",
    "            print(f'No influence rating for {subj_id}')\n",
    "        continue\n",
    "\n",
    "preproc_dfs['Hardball'] = preproc_dfs['Hardball'].set_index('index')\n",
    "preproc_dfs['Hardball'].to_csv(f'{out_dir}/Hardball-data-{todays_date}.csv', index_label='SubjectID')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Journey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68a59a4a271b255df02ddff89ceeff0b multiple task versions\n"
     ]
    }
   ],
   "source": [
    "for journey in ['Journey_decisions', 'Journey_memory', 'Journey_characters']:\n",
    "    df = full_dfs[journey].copy().reset_index()\n",
    "    preproc_dfs[journey] = df.rename(columns={'index': 'sub_id'})\n",
    "\n",
    "# remove journey subjects with less than 63 completed trials\n",
    "snt = []\n",
    "ver = []\n",
    "mem = []\n",
    "for sub_id in np.unique(full_dfs['Journey_decisions'].index):\n",
    "    \n",
    "    ntrials = len(full_dfs['Journey_decisions'][full_dfs['Journey_decisions'].index == sub_id])\n",
    "    \n",
    "    # preprocess anyone w/ 63 or more trials \n",
    "    if ntrials >= 63:\n",
    "        \n",
    "        sub_df = full_dfs['Journey_decisions'][full_dfs['Journey_decisions'].index == sub_id].copy()\n",
    "        sub_df['Num_trials'] = ntrials\n",
    "        sub_df['Include'] = 1\n",
    "        \n",
    "        # if two trials are > 30 minutes apart define them as separate sessions...\n",
    "        s = 1\n",
    "        sessions = [[1, 0]]\n",
    "        datetimes = [datetime.strptime(dt, \"%Y-%m-%d %H:%M:%S.%f\") for dt in sub_df['Datetime'].values]\n",
    "        for d, dt in enumerate(datetimes[1:]):\n",
    "            elapsed = dt - datetimes[d]\n",
    "            if elapsed.seconds > (60 * 30): \n",
    "                s += 1\n",
    "            sessions.append([s, elapsed.seconds])\n",
    "        sub_df[['Session', 'Elapsed(s)']] = sessions\n",
    "        \n",
    "        # count number of repeats for each decision\n",
    "        counts = sub_df['decision_num'].value_counts()\n",
    "        sub_df['Num_repeats'] = [counts[c] for c in sub_df['decision_num'].values]\n",
    "        \n",
    "        # put together\n",
    "        sub_ver = full_dfs['Journey_characters'][full_dfs['Journey_characters'].index == sub_id].copy()\n",
    "        if len(sub_ver) > 1: \n",
    "            print(sub_id + ' multiple task versions')\n",
    "            sub_df['Mult_versions'] = 1\n",
    "        else:\n",
    "            sub_df['Mult_versions'] = 0\n",
    "            \n",
    "        snt.append(sub_df)\n",
    "        ver.append(sub_ver)\n",
    "        mem.append(full_dfs['Journey_memory'][full_dfs['Journey_memory'].index == sub_id].copy())\n",
    "\n",
    "# output\n",
    "pd.concat(snt).to_csv(f'{out_dir}/SNT_data_{todays_date}.csv', index_label='sub_id')\n",
    "pd.concat(mem).to_csv(f'{out_dir}/SNT-memory_data_{todays_date}.csv', index_label='sub_id')\n",
    "pd.concat(ver).to_csv(f'{out_dir}/SNT-ver_data_{todays_date}.csv', index_label='sub_id')\n",
    "\n",
    "# TO DO: output role of the characters selected in memory by referencing ver "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demographics + Questionnaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AECC8FE9-F873-4EC2-B4BA-40386726DC33 41\n"
     ]
    }
   ],
   "source": [
    "full_dfs['OCI'].to_csv(f'{out_dir}/OCI-rawdata-{todays_date}.csv', index_label='SubjectID')\n",
    "\n",
    "# remove OCI subjects with less than 19 completed trials\n",
    "tmp_dflist = []\n",
    "for subj_id in np.unique(full_dfs['OCI'].index):\n",
    "    subj_df = full_dfs['OCI'].loc[subj_id]\n",
    "    if len(subj_df) == 19: # completed survey once\n",
    "        # grab attention check\n",
    "        attn_check = float(subj_df[subj_df['SurveyQuestion']==15]['Response'])\n",
    "\n",
    "        # long to wide, exclude attention check\n",
    "        subj_df = subj_df[subj_df['SurveyQuestion']!=15].pivot(columns='SurveyQuestion', values='Response').add_prefix('OCI_')\n",
    "\n",
    "        subj_df['OCI_Total'] = subj_df.sum(axis=1, skipna=False)\n",
    "\n",
    "        if np.isnan(attn_check):\n",
    "            subj_df['OCI_AttnCheck'] = 1\n",
    "        else:\n",
    "            subj_df['OCI_AttnCheck'] = 0\n",
    "\n",
    "        tmp_dflist += [subj_df]\n",
    "    elif len(subj_df) > 19:\n",
    "        print(subj_id, len(subj_df))\n",
    "\n",
    "preproc_dfs['OCI'] = pd.concat(tmp_dflist)\n",
    "preproc_dfs['OCI'].to_csv(f'{out_dir}/OCI-data-{todays_date}.csv', index_label='SubjectID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2FCBC76A-8621-427C-9197-3B0241D06206 22\n",
      "405F5765-F99C-4A61-9D21-55C4BCD818B0 1701\n"
     ]
    }
   ],
   "source": [
    "full_dfs['SDS'].to_csv(f'{out_dir}/SDS-rawdata-{todays_date}.csv', index_label='SubjectID')\n",
    "\n",
    "# remove SDS subjects with less than 21 completed trials\n",
    "tmp_dflist = []\n",
    "for subj_id in np.unique(full_dfs['SDS'].index):\n",
    "    subj_df = full_dfs['SDS'].loc[subj_id]\n",
    "    if len(subj_df) == 21: # completed survey once\n",
    "        # grab attention check\n",
    "        attn_check = float(subj_df[subj_df['SurveyQuestion']==16]['Response'])\n",
    "\n",
    "        # long to wide, exclude attention check\n",
    "        subj_df = subj_df[subj_df['SurveyQuestion']!=16].pivot(columns='SurveyQuestion', values='Response').add_prefix('SDS_')\n",
    "\n",
    "        subj_df['SDS_Total'] = subj_df.sum(axis=1, skipna=False)\n",
    "\n",
    "        if np.isnan(attn_check):\n",
    "            subj_df['SDS_AttnCheck'] = 1\n",
    "        else:\n",
    "            subj_df['SDS_AttnCheck'] = 0\n",
    "\n",
    "        tmp_dflist += [subj_df]\n",
    "    elif len(subj_df) > 21:\n",
    "        print(subj_id, len(subj_df))\n",
    "\n",
    "preproc_dfs['SDS'] = pd.concat(tmp_dflist)\n",
    "preproc_dfs['SDS'].to_csv(f'{out_dir}/SDS-data-{todays_date}.csv', index_label='SubjectID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1F3D051E-8385-4C5A-9627-B30383846268 25\n",
      "20386890-DC58-451B-95D4-D8873C33D195 26\n",
      "309399CE-63A7-472E-8619-65576780E6A2 31\n",
      "405F5765-F99C-4A61-9D21-55C4BCD818B0 474\n",
      "68a59a4a271b255df02ddff89ceeff0b 26\n"
     ]
    }
   ],
   "source": [
    "full_dfs['LSAS'].to_csv(f'{out_dir}/LSAS-rawdata-{todays_date}.csv', index_label='SubjectID')\n",
    "\n",
    "# remove LSAS subjects with less than 24 completed trials\n",
    "tmp_dflist = []\n",
    "for subj_id in np.unique(full_dfs['LSAS'].index):\n",
    "    subj_df = full_dfs['LSAS'].loc[subj_id]\n",
    "    if len(subj_df) == 24: # completed survey once\n",
    "        # long to wide, exclude attention check\n",
    "        subj_df = subj_df.pivot(columns='SurveyQuestion', values='Response').add_prefix('LSAS_')\n",
    "\n",
    "        subj_df['LSAS_Total'] = subj_df.sum(axis=1, skipna=False)\n",
    "\n",
    "        tmp_dflist += [subj_df]\n",
    "    elif len(subj_df) > 24:\n",
    "        print(subj_id, len(subj_df))\n",
    "\n",
    "preproc_dfs['LSAS'] = pd.concat(tmp_dflist)\n",
    "preproc_dfs['LSAS'].to_csv(f'{out_dir}/LSAS-data-{todays_date}.csv', index_label='SubjectID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save demographics\n",
    "preproc_dfs['Demographics'] = full_dfs['Demographics'].copy()\n",
    "preproc_dfs['Demographics'] = preproc_dfs['Demographics'].join(preproc_dfs['LSAS'])\n",
    "preproc_dfs['Demographics'] = preproc_dfs['Demographics'].join(preproc_dfs['OCI'])\n",
    "preproc_dfs['Demographics'] = preproc_dfs['Demographics'].join(preproc_dfs['SDS'])\n",
    "preproc_dfs['Demographics'].to_csv(f'{out_dir}/Demographics-data-{todays_date}.csv', index_label='SubjectID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as pickle\n",
    "with open(f'{out_dir}/SocialBrainAppData-{todays_date}.pickle', 'wb') as handle:\n",
    "    pickle.dump(preproc_dfs, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move files after preprocessing is completed\n",
    "for json_file in json_files:\n",
    "    os.rename(json_file, f'{json_dir}/processed/{os.path.basename(json_file)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of New Unique IDs with Any Data\n",
      "Demographics 28\n",
      "Hardball 23\n",
      "HardballSubjectiveRatings 20\n",
      "OCI 22\n",
      "SDS 23\n",
      "LSAS 14\n",
      "Journey_decisions 32\n",
      "Journey_memory 31\n",
      "Journey_dots 0\n",
      "Journey_characters 33\n"
     ]
    }
   ],
   "source": [
    "print('Number of New Unique IDs with Any Data')\n",
    "for name, id_list in id_lists.items():\n",
    "    print(name, len(np.unique(id_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of New Unique IDs with Complete Data\n",
      "Hardball 20\n",
      "HardballSubjectiveRatings 20\n",
      "Journey_decisions 1120\n",
      "Journey_memory 328\n",
      "Journey_characters 37\n",
      "OCI 21\n",
      "SDS 20\n",
      "LSAS 9\n",
      "Demographics 28\n"
     ]
    }
   ],
   "source": [
    "print('Number of New Unique IDs with Complete Data')\n",
    "for name, id_list in preproc_dfs.items():\n",
    "    print(name, len(np.unique(id_list.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Append new data to Master "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8312DACF-5F5F-4BEC-9AC8-2F3CE0D0C47E\n",
      "A428C902-3440-4AA8-9617-56BA7D16277D\n",
      "E98C656F-137B-4603-9934-7697BF73323D\n"
     ]
    }
   ],
   "source": [
    "# append to Master csvs\n",
    "preproc_dfs['Hardball'] = preproc_dfs['Hardball'].rename_axis('SubjectID')\n",
    "preproc_dfs['Demographics'] = preproc_dfs['Demographics'].rename_axis('SubjectID')\n",
    "\n",
    "master_dfs = {'Hardball': pd.read_csv('../data/Hardball-data-Master.csv', index_col='SubjectID'), \n",
    "              'Demographics': pd.read_csv('../data/Demographics-data-Master.csv', index_col='SubjectID')}\n",
    "\n",
    "for subj_id in list(np.unique(preproc_dfs['Hardball'].index)):\n",
    "    if subj_id in list(np.unique(master_dfs['Hardball'].index)):\n",
    "        print(subj_id)\n",
    "        preproc_dfs['Hardball'].loc[subj_id,'Include'] = 0.0\n",
    "        preproc_dfs['Hardball'].loc[subj_id,'Notes'] = 'Participated previously'\n",
    "\n",
    "master_dfs['Hardball'] = master_dfs['Hardball'].append(preproc_dfs['Hardball'])\n",
    "master_dfs['Hardball'].to_csv('../data/Hardball-data-Master2.csv', index_label='SubjectID')\n",
    "\n",
    "master_dfs['Demographics'] = master_dfs['Demographics'].append(preproc_dfs['Demographics'])\n",
    "master_dfs['Demographics'].to_csv('../data/Demographics-data-Master2.csv', index_label='SubjectID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Unique IDs with Complete Data\n",
      "Hardball 122\n",
      "Demographics 333\n"
     ]
    }
   ],
   "source": [
    "print('Total Number of Unique IDs with Complete Data')\n",
    "for name, master_df in master_dfs.items():\n",
    "    if name == 'Hardball':\n",
    "        print(name, len(np.unique(master_df[master_df.Include==1].index)))\n",
    "    else:\n",
    "        print(name, len(np.unique(master_df.index)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
