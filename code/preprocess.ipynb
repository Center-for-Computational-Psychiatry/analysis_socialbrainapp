{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, json, pickle, os, warnings\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "\n",
    "from socialbrainapp import get_demographics, get_oci, get_sds, get_lsas, get_hardball, get_hardball_ratings, get_hardball_blocks, get_hardball_sessions, get_journey, get_withdrawn_ids\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get today's date\n",
    "todays_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "# Load json data\n",
    "json_dir = '../json'\n",
    "out_dir = '../data/runs'\n",
    "json_files = glob(f'{json_dir}/*json')\n",
    "json_files.sort()\n",
    "print(f'{len(json_files)} jsons to parse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse json data\n",
    "id_lists = {'Demographics':[], 'Hardball':[], 'HardballSubjectiveRatings':[], \n",
    "            'OCI':[], 'SDS':[], 'LSAS':[], \n",
    "            'Journey_decisions':[], 'Journey_memory':[], 'Journey_dots':[], 'Journey_characters':[]}\n",
    "\n",
    "df_lists = {'Demographics':[], 'Hardball':[], 'HardballSubjectiveRatings':[], \n",
    "            'OCI':[], 'SDS':[], 'LSAS':[],\n",
    "            'Journey_decisions':[], 'Journey_memory':[], 'Journey_dots':[], 'Journey_characters':[]}\n",
    "\n",
    "withdraw_ids = []\n",
    "withdrawshare_ids = []\n",
    "for json_file in json_files:\n",
    "    \n",
    "    try:\n",
    "        with open(json_file, 'rb') as handle:\n",
    "            tmp_data = json.load(handle)\n",
    "\n",
    "        for element in tmp_data:\n",
    "\n",
    "            # Check if new version\n",
    "            if \"AppVersion\" in element: \n",
    "                VersionNum = element[\"AppVersion\"]\n",
    "            else:\n",
    "                VersionNum = \"1.0\"\n",
    "\n",
    "            # Check for withdrawal\n",
    "            if 'Withdrawal' in element.keys():\n",
    "                if element['Withdrawal'] == 'True':\n",
    "                    withdraw_ids.append(element['UserId'])\n",
    "            \n",
    "            if 'WithdrawalShare' in element.keys():\n",
    "                if element['WithdrawalShare'] == 'True':\n",
    "                    withdrawshare_ids.append(element['UserId'])\n",
    "\n",
    "            # Get Demographics\n",
    "            if 'Age' in element.keys():\n",
    "                id_lists['Demographics'] += [element['UserId']]\n",
    "                df_lists['Demographics'] += [get_demographics(element)]\n",
    "\n",
    "            # Get Survey Data\n",
    "            if 'SurveyName' in element.keys():\n",
    "\n",
    "                # Get OCI\n",
    "                if element['SurveyName'] == 'OCI':\n",
    "                    id_lists['OCI'] += [element['UserId']]\n",
    "                    df_lists['OCI'] += [get_oci(element)]\n",
    "\n",
    "                # Get SDS\n",
    "                elif element['SurveyName'] == 'SDS':\n",
    "                    id_lists['SDS'] += [element['UserId']]\n",
    "                    df_lists['SDS'] += [get_sds(element)]\n",
    "\n",
    "                # Get LSAS\n",
    "                elif element['SurveyName'] == 'LSAS':\n",
    "                    id_lists['LSAS'] += [element['UserId']]\n",
    "                    df_lists['LSAS'] += [get_lsas(element)]\n",
    "\n",
    "            # Get Game Data\n",
    "            if 'Game' in element.keys():\n",
    "                \n",
    "                # Get Hardball\n",
    "                if element['Game'] == 'Hardball':\n",
    "                    if 'Screen' not in element.keys():\n",
    "                        id_lists['Hardball'] += [element['UserId']]\n",
    "                        df_lists['Hardball'] += [get_hardball(element)]\n",
    "\n",
    "                    if 'Screen' in element.keys():\n",
    "                        id_lists['HardballSubjectiveRatings'] += [element['UserId']]\n",
    "                        df_lists['HardballSubjectiveRatings'] += [get_hardball_ratings(element)]\n",
    "\n",
    "                # Get Journey\n",
    "                if element['Game'] == 'Journey':\n",
    "                    \n",
    "                    try: \n",
    "                        task_name, snt_df = get_journey(element)\n",
    "                        id_lists['Journey_'+task_name] += [element['UserId']]\n",
    "                        df_lists['Journey_'+task_name] += [snt_df]\n",
    "                        \n",
    "                    except: # not interested in some trials but want a better solution here\n",
    "                        continue\n",
    "\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# concatenate all df lists\n",
    "full_dfs = {}\n",
    "for (key, value) in df_lists.items():\n",
    "    if len(value) > 0: \n",
    "        full_dfs[key] = []\n",
    "        full_dfs[key] = pd.concat(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_withdraw_noshare, withdraw_only, noshare_only = get_withdrawn_ids(withdraw_ids, withdrawshare_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Data\n",
    "preproc_dfs = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dfs['Hardball'] = full_dfs['Hardball'].drop_duplicates()\n",
    "full_dfs['HardballSubjectiveRatings'] = full_dfs['HardballSubjectiveRatings'].drop_duplicates()\n",
    "\n",
    "Hardball_df_list       = []\n",
    "HardballRating_df_list = []\n",
    "for subj_id in np.unique(full_dfs['Hardball'].index):\n",
    "    # Get subject data and then sort in time\n",
    "    subj_hardball_df = full_dfs['Hardball'][full_dfs['Hardball'].index==subj_id]\n",
    "    subj_hardball_df = subj_hardball_df.sort_values(by=['Year', 'Month', 'Day', 'Hour', 'Minute', 'Second']).sort_index()\n",
    "\n",
    "    # Get subjective ratings\n",
    "    subj_ratings_df = full_dfs['HardballSubjectiveRatings'][full_dfs['HardballSubjectiveRatings'].index==subj_id]\n",
    "\n",
    "    # Check if withdrawn from study\n",
    "    if subj_id in both_withdraw_noshare:\n",
    "        subj_hardball_df.at[subj_id, 'WithdrawType'] = 'FromStudy'\n",
    "        subj_hardball_df.at[subj_id, 'WithdrawAll'] = 1\n",
    "        subj_hardball_df.at[subj_id, 'WithdrawStudy'] = 0\n",
    "        subj_hardball_df.at[subj_id, 'WithdrawData'] = 0\n",
    "    \n",
    "    elif subj_id in withdraw_only:\n",
    "        subj_hardball_df.at[subj_id, 'WithdrawType'] = 'FromStudy'\n",
    "        subj_hardball_df.at[subj_id, 'WithdrawAll'] = 0\n",
    "        subj_hardball_df.at[subj_id, 'WithdrawStudy'] = 1\n",
    "        subj_hardball_df.at[subj_id, 'WithdrawData'] = 0\n",
    "\n",
    "    elif subj_id in noshare_only:\n",
    "        subj_hardball_df.at[subj_id, 'WithdrawType'] = 'Data'\n",
    "        subj_hardball_df.at[subj_id, 'WithdrawAll'] = 0\n",
    "        subj_hardball_df.at[subj_id, 'WithdrawStudy'] = 0\n",
    "        subj_hardball_df.at[subj_id, 'WithdrawData'] = 1\n",
    "\n",
    "    else:\n",
    "        subj_hardball_df.at[subj_id, 'WithdrawType'] = 'NA'\n",
    "        subj_hardball_df.at[subj_id, 'WithdrawAll'] = 0\n",
    "        subj_hardball_df.at[subj_id, 'WithdrawStudy'] = 0\n",
    "        subj_hardball_df.at[subj_id, 'WithdrawData'] = 0\n",
    "\n",
    "    ntrials = len(subj_hardball_df)\n",
    "    if ntrials < 60: # not completed\n",
    "        continue\n",
    "        \n",
    "    else: # 60 trials or more\n",
    "        subj_hardball_df.at[subj_id, 'NTrials'] = ntrials\n",
    "\n",
    "        # identify blocks for each subject\n",
    "        subj_hardball_df_blocks = get_hardball_blocks(subj_hardball_df.reset_index())\n",
    "        subj_hardball_df_sessions = get_hardball_sessions(subj_hardball_df_blocks)\n",
    "\n",
    "        # Add to list of dfs:\n",
    "        Hardball_df_list       += [subj_hardball_df_sessions]\n",
    "        HardballRating_df_list += [subj_ratings_df]\n",
    "\n",
    "# combine subject dfs\n",
    "preproc_dfs['Hardball'] = pd.concat(Hardball_df_list)\n",
    "preproc_dfs['Hardball'] = preproc_dfs['Hardball'].reset_index()\n",
    "preproc_dfs['Hardball']['DateTime'] = pd.to_datetime(preproc_dfs['Hardball'][['Year', 'Month', 'Day', 'Hour', 'Minute', 'Second']])\n",
    "\n",
    "preproc_dfs['HardballSubjectiveRatings'] = pd.concat(HardballRating_df_list)\n",
    "preproc_dfs['HardballSubjectiveRatings'] = preproc_dfs['HardballSubjectiveRatings'].reset_index()\n",
    "preproc_dfs['HardballSubjectiveRatings']['DateTime'] = pd.to_datetime(preproc_dfs['HardballSubjectiveRatings'][['Year', 'Month', 'Day', 'Hour', 'Minute', 'Second']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column in df2 to store the Rate values from df1\n",
    "preproc_dfs['Hardball']['PerceivedControl'] = np.nan\n",
    "preproc_dfs['Hardball']['RateDateTime'] = np.nan\n",
    "\n",
    "# Loop through rows in df1\n",
    "for i, row in preproc_dfs['HardballSubjectiveRatings'].iterrows():\n",
    "    subj_id = row['index']\n",
    "    team_id = row['TeamName']\n",
    "\n",
    "    # Find the SessionID in df2 that is closest in time to the current row\n",
    "    # TO DO : Find sessionID that is earlier for behav\n",
    "    behav_time1 = preproc_dfs['Hardball'][preproc_dfs['Hardball']['index']==subj_id]['DateTime']\n",
    "    idx = abs(row['DateTime'] - behav_time1[behav_time1<row['DateTime']]).idxmin()\n",
    "\n",
    "    # idx = abs(preproc_dfs['Hardball'][preproc_dfs['Hardball']['index']==subj_id][preproc_dfs['Hardball']['DateTime']<row['DateTime']]['DateTime'] - row['DateTime']).idxmin()\n",
    "    \n",
    "    sess_id = preproc_dfs['Hardball'][preproc_dfs['Hardball']['index']==subj_id].at[idx, 'SessionID']\n",
    "\n",
    "    # Write df1['Rate'] values for all matching index, TeamName, and BlockID\n",
    "    rows_meeting_criteria = (preproc_dfs['Hardball']['index'] == subj_id) & (preproc_dfs['Hardball']['TeamName'] == team_id) & (preproc_dfs['Hardball']['SessionID'] == sess_id)\n",
    "\n",
    "    if all(preproc_dfs['Hardball'].loc[rows_meeting_criteria, 'PerceivedControl'].isna()):\n",
    "        preproc_dfs['Hardball'].loc[rows_meeting_criteria, 'PerceivedControl'] = row['Rate']\n",
    "        preproc_dfs['Hardball'].loc[rows_meeting_criteria, 'RateDateTime'] = row['DateTime']\n",
    "    else:\n",
    "        # check which is closer to behav\n",
    "        behav_time = preproc_dfs['Hardball'].loc[rows_meeting_criteria, 'DateTime'].iloc[-1]\n",
    "        prev_rating_time = preproc_dfs['Hardball'].loc[rows_meeting_criteria, 'RateDateTime'].iloc[0]\n",
    "        new_rating_time = row['DateTime']\n",
    "\n",
    "        if abs(prev_rating_time - behav_time) < abs(new_rating_time - behav_time):\n",
    "            # overwrite\n",
    "            preproc_dfs['Hardball'].loc[rows_meeting_criteria, 'PerceivedControl'] = row['Rate']\n",
    "            preproc_dfs['Hardball'].loc[rows_meeting_criteria, 'RateDateTime'] = row['DateTime']\n",
    "        else: \n",
    "            print(subj_id)\n",
    "            # put in other column\n",
    "            preproc_dfs['Hardball'].loc[rows_meeting_criteria, 'PerceivedControl2'] = row['Rate']\n",
    "            preproc_dfs['Hardball'].loc[rows_meeting_criteria, 'Rate2DateTimeTime'] = row['DateTime']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_dfs['Hardball'] = preproc_dfs['Hardball'].rename({'index':'SubjectID','level_0':'TrialNum'},axis='columns')\n",
    "preproc_dfs['Hardball'].to_csv(f'{out_dir}/Hardball-data-{todays_date}.csv', index_label='Index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Journey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for journey in ['Journey_decisions', 'Journey_memory', 'Journey_characters']:\n",
    "    df = full_dfs[journey].copy().reset_index()\n",
    "    preproc_dfs[journey] = df.rename(columns={'index': 'sub_id'})\n",
    "\n",
    "# remove journey subjects with less than 63 completed trials\n",
    "snt = []\n",
    "ver = []\n",
    "mem = []\n",
    "for sub_id in np.unique(full_dfs['Journey_decisions'].index):\n",
    "    \n",
    "    ntrials = len(full_dfs['Journey_decisions'][full_dfs['Journey_decisions'].index == sub_id])\n",
    "    \n",
    "    # preprocess anyone w/ 63 or more trials \n",
    "    if ntrials >= 63:\n",
    "        \n",
    "        sub_df = full_dfs['Journey_decisions'][full_dfs['Journey_decisions'].index == sub_id].copy()\n",
    "        sub_df['Num_trials'] = ntrials\n",
    "        sub_df['Include'] = 1\n",
    "        \n",
    "        # if two trials are > 30 minutes apart define them as separate sessions...\n",
    "        s = 1\n",
    "        sessions = [[1, 0]]\n",
    "        datetimes = [datetime.strptime(dt, \"%Y-%m-%d %H:%M:%S.%f\") for dt in sub_df['Datetime'].values]\n",
    "        for d, dt in enumerate(datetimes[1:]):\n",
    "            elapsed = dt - datetimes[d]\n",
    "            if elapsed.seconds > (60 * 30): \n",
    "                s += 1\n",
    "            sessions.append([s, elapsed.seconds])\n",
    "        sub_df[['Session', 'Elapsed(s)']] = sessions\n",
    "        \n",
    "        # count number of repeats for each decision\n",
    "        counts = sub_df['decision_num'].value_counts()\n",
    "        sub_df['Num_repeats'] = [counts[c] for c in sub_df['decision_num'].values]\n",
    "        \n",
    "        # put together\n",
    "        sub_ver = full_dfs['Journey_characters'][full_dfs['Journey_characters'].index == sub_id].copy()\n",
    "        if len(sub_ver) > 1: \n",
    "            print(sub_id + ' multiple task versions')\n",
    "            sub_df['Mult_versions'] = 1\n",
    "        else:\n",
    "            sub_df['Mult_versions'] = 0\n",
    "            \n",
    "        snt.append(sub_df)\n",
    "        ver.append(sub_ver)\n",
    "        mem.append(full_dfs['Journey_memory'][full_dfs['Journey_memory'].index == sub_id].copy())\n",
    "\n",
    "# output\n",
    "pd.concat(snt).to_csv(f'{out_dir}/SNT_data_{todays_date}.csv', index_label='sub_id')\n",
    "pd.concat(mem).to_csv(f'{out_dir}/SNT-memory_data_{todays_date}.csv', index_label='sub_id')\n",
    "pd.concat(ver).to_csv(f'{out_dir}/SNT-ver_data_{todays_date}.csv', index_label='sub_id')\n",
    "\n",
    "# TO DO: output role of the characters selected in memory by referencing ver "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demographics + Questionnaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dfs['OCI'].to_csv(f'{out_dir}/OCI-rawdata-{todays_date}.csv', index_label='SubjectID')\n",
    "\n",
    "# remove OCI subjects with less than 19 completed trials\n",
    "tmp_dflist = []\n",
    "for subj_id in np.unique(full_dfs['OCI'].index):\n",
    "    subj_df = full_dfs['OCI'].loc[subj_id]\n",
    "    if len(np.unique(subj_df['SurveyQuestion'])) == 19: # completed survey once\n",
    "        # grab attention check\n",
    "        attn_check = float(subj_df[subj_df['SurveyQuestion']==15]['Response'])\n",
    "\n",
    "        # long to wide, exclude attention check\n",
    "        subj_df = subj_df[subj_df['SurveyQuestion']!=15].drop_duplicates().pivot(columns='SurveyQuestion', values='Response').add_prefix('OCI_')\n",
    "\n",
    "        subj_df['OCI_Total'] = subj_df.sum(axis=1, skipna=False)\n",
    "\n",
    "        if np.isnan(attn_check):\n",
    "            subj_df['OCI_AttnCheck'] = 1\n",
    "        else:\n",
    "            subj_df['OCI_AttnCheck'] = 0\n",
    "\n",
    "        tmp_dflist += [subj_df]\n",
    "    elif len(subj_df) > 19:\n",
    "        print(subj_id, len(subj_df))\n",
    "\n",
    "preproc_dfs['OCI'] = pd.concat(tmp_dflist)\n",
    "preproc_dfs['OCI'].to_csv(f'{out_dir}/OCI-data-{todays_date}.csv', index_label='SubjectID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dfs['SDS'].to_csv(f'{out_dir}/SDS-rawdata-{todays_date}.csv', index_label='SubjectID')\n",
    "\n",
    "# remove SDS subjects with less than 21 completed trials\n",
    "tmp_dflist = []\n",
    "for subj_id in np.unique(full_dfs['SDS'].index):\n",
    "    subj_df = full_dfs['SDS'].loc[subj_id]\n",
    "    if len(np.unique(subj_df['SurveyQuestion'])) == 21: # completed survey once\n",
    "        # grab attention check\n",
    "        attn_check = float(subj_df[subj_df['SurveyQuestion']==16]['Response'])\n",
    "\n",
    "        # long to wide, exclude attention check\n",
    "        subj_df = subj_df[subj_df['SurveyQuestion']!=16].drop_duplicates().pivot(columns='SurveyQuestion', values='Response').add_prefix('SDS_')\n",
    "\n",
    "        subj_df['SDS_Total'] = subj_df.sum(axis=1, skipna=False)\n",
    "\n",
    "        if np.isnan(attn_check):\n",
    "            subj_df['SDS_AttnCheck'] = 1\n",
    "        else:\n",
    "            subj_df['SDS_AttnCheck'] = 0\n",
    "\n",
    "        tmp_dflist += [subj_df]\n",
    "    elif len(subj_df) > 21:\n",
    "        print(subj_id, len(subj_df))\n",
    "\n",
    "preproc_dfs['SDS'] = pd.concat(tmp_dflist)\n",
    "preproc_dfs['SDS'].to_csv(f'{out_dir}/SDS-data-{todays_date}.csv', index_label='SubjectID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dfs['LSAS'].to_csv(f'{out_dir}/LSAS-rawdata-{todays_date}.csv', index_label='SubjectID')\n",
    "\n",
    "# remove LSAS subjects with less than 24 completed trials\n",
    "tmp_dflist = []\n",
    "for subj_id in np.unique(full_dfs['LSAS'].index):\n",
    "    subj_df = full_dfs['LSAS'].loc[subj_id]\n",
    "    if len(np.unique(subj_df['SurveyQuestion'])) == 24: # completed survey once\n",
    "        # long to wide, exclude attention check\n",
    "        subj_df = subj_df.drop_duplicates(subset=['SurveyQuestion']).pivot(columns='SurveyQuestion', values='Response').add_prefix('LSAS_')\n",
    "\n",
    "        subj_df['LSAS_Total'] = subj_df.sum(axis=1, skipna=False)\n",
    "\n",
    "        tmp_dflist += [subj_df]\n",
    "    elif len(subj_df) > 24:\n",
    "        print(subj_id, len(subj_df))\n",
    "\n",
    "preproc_dfs['LSAS'] = pd.concat(tmp_dflist)\n",
    "preproc_dfs['LSAS'].to_csv(f'{out_dir}/LSAS-data-{todays_date}.csv', index_label='SubjectID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hardball_sess1 = preproc_dfs['Hardball'][preproc_dfs['Hardball']['SessionID']==1]\n",
    "hardball_sess1 = hardball_sess1[hardball_sess1['OpponentNum']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hardball_sess1_IC = hardball_sess1[hardball_sess1['Condition']==\"Control\"]\n",
    "hardball_sess1_IC = hardball_sess1_IC.rename({'PerceivedControl':'PerceivedControl_IC'},axis='columns')\n",
    "hardball_sess1_IC.set_index('SubjectID',inplace=True)\n",
    "\n",
    "hardball_sess1_NC = hardball_sess1[hardball_sess1['Condition']==\"NoControl\"]\n",
    "hardball_sess1_NC = hardball_sess1_NC.rename({'PerceivedControl':'PerceivedControl_NC'},axis='columns')\n",
    "hardball_sess1_NC.set_index('SubjectID',inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save demographics\n",
    "preproc_dfs['Demographics'] = full_dfs['Demographics'].drop_duplicates().copy()\n",
    "preproc_dfs['Demographics'] = preproc_dfs['Demographics'].join(preproc_dfs['LSAS'])\n",
    "preproc_dfs['Demographics'] = preproc_dfs['Demographics'].join(preproc_dfs['OCI'])\n",
    "preproc_dfs['Demographics'] = preproc_dfs['Demographics'].join(preproc_dfs['SDS'])\n",
    "preproc_dfs['Demographics'] = preproc_dfs['Demographics'].join(hardball_sess1_NC['PerceivedControl_NC'])\n",
    "preproc_dfs['Demographics'] = preproc_dfs['Demographics'].join(hardball_sess1_IC['PerceivedControl_IC'])\n",
    "\n",
    "for subj_id in np.unique(preproc_dfs['Demographics'].index):\n",
    "    \n",
    "    # Check if withdrawn from study\n",
    "    if subj_id in both_withdraw_noshare:\n",
    "        preproc_dfs['Demographics'].at[subj_id, 'WithdrawType'] = 'FromStudy'\n",
    "        preproc_dfs['Demographics'].at[subj_id, 'WithdrawAll'] = 1\n",
    "        preproc_dfs['Demographics'].at[subj_id, 'WithdrawStudy'] = 0\n",
    "        preproc_dfs['Demographics'].at[subj_id, 'WithdrawData'] = 0\n",
    "    \n",
    "    elif subj_id in withdraw_only:\n",
    "        preproc_dfs['Demographics'].at[subj_id, 'WithdrawType'] = 'FromStudy'\n",
    "        preproc_dfs['Demographics'].at[subj_id, 'WithdrawAll'] = 0\n",
    "        preproc_dfs['Demographics'].at[subj_id, 'WithdrawStudy'] = 1\n",
    "        preproc_dfs['Demographics'].at[subj_id, 'WithdrawData'] = 0\n",
    "\n",
    "    elif subj_id in noshare_only:\n",
    "        preproc_dfs['Demographics'].at[subj_id, 'WithdrawType'] = 'Data'\n",
    "        preproc_dfs['Demographics'].at[subj_id, 'WithdrawAll'] = 0\n",
    "        preproc_dfs['Demographics'].at[subj_id, 'WithdrawStudy'] = 0\n",
    "        preproc_dfs['Demographics'].at[subj_id, 'WithdrawData'] = 1\n",
    "\n",
    "    else:\n",
    "        preproc_dfs['Demographics'].at[subj_id, 'WithdrawType'] = 'NA'\n",
    "        preproc_dfs['Demographics'].at[subj_id, 'WithdrawAll'] = 0\n",
    "        preproc_dfs['Demographics'].at[subj_id, 'WithdrawStudy'] = 0\n",
    "        preproc_dfs['Demographics'].at[subj_id, 'WithdrawData'] = 0\n",
    "\n",
    "preproc_dfs['Demographics'].to_csv(f'{out_dir}/Demographics-data-{todays_date}.csv', index_label='SubjectID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as pickle\n",
    "with open(f'{out_dir}/SocialBrainAppData-{todays_date}.pickle', 'wb') as handle:\n",
    "    pickle.dump(preproc_dfs, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move files after preprocessing is completed\n",
    "for json_file in json_files[1:]:\n",
    "    os.rename(json_file, f'{json_dir}/processed/{os.path.basename(json_file)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of New Unique IDs with Any Data')\n",
    "for name, id_list in id_lists.items():\n",
    "    print(name, len(np.unique(id_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of New Unique IDs with Complete Data')\n",
    "for name, id_list in preproc_dfs.items():\n",
    "    if name == 'Hardball':\n",
    "        print(name, len(np.unique(id_list['SubjectID'])))\n",
    "    elif name == 'HardballSubjectiveRatings':\n",
    "        print(name, len(np.unique(id_list['index'])))\n",
    "    else:\n",
    "        print(name, len(np.unique(id_list.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "preproc_dfs['Demographics']['Date'] = pd.to_datetime(preproc_dfs['Demographics'][['Year','Month','Day']])\n",
    "dates_df = preproc_dfs['Demographics'].groupby(['Date']).size()\n",
    "\n",
    "plt.figure(figsize=(15,3))\n",
    "ax = dates_df.plot()\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.savefig(f'counts_{todays_date}.jpg',bbox_inches='tight',dpi=450)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
